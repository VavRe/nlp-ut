{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m comment \u001b[39min\u001b[39;00m comments:\n\u001b[1;32m      4\u001b[0m     comment \u001b[39m=\u001b[39m hz\u001b[39m.\u001b[39mNormalizer()\u001b[39m.\u001b[39mnormalize(comment)\n\u001b[0;32m----> 5\u001b[0m     comment \u001b[39m=\u001b[39m hz\u001b[39m.\u001b[39;49mWordTokenizer()\u001b[39m.\u001b[39mtokenize(comment)\n\u001b[1;32m      6\u001b[0m dataset[\u001b[39m\"\u001b[39m\u001b[39mcomment\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m comments\n\u001b[1;32m      7\u001b[0m dataset\n",
      "File \u001b[0;32m~/nlp/hazm-venv/lib/python3.10/site-packages/hazm/WordTokenizer.py:70\u001b[0m, in \u001b[0;36mWordTokenizer.__init__\u001b[0;34m(self, words_file, verbs_file, join_verb_parts, separate_emoji, replace_links, replace_IDs, replace_emails, replace_numbers, replace_hashtags)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39m# NOTE: python2.7 does not support unicodes with \\w  Example: r'\\#([\\w\\_]+)'\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhashtag_repl \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m m: \u001b[39m'\u001b[39m\u001b[39mTAG \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m m\u001b[39m.\u001b[39mgroup(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 70\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwords \u001b[39m=\u001b[39m {item[\u001b[39m0\u001b[39m]: (item[\u001b[39m1\u001b[39m], item[\u001b[39m2\u001b[39m]) \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m words_list(default_words)}\n\u001b[1;32m     72\u001b[0m \u001b[39mif\u001b[39;00m join_verb_parts:\n\u001b[1;32m     73\u001b[0m \t\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter_verbs \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m([\n\u001b[1;32m     74\u001b[0m \t\t\u001b[39m'\u001b[39m\u001b[39mام\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mای\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mاست\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mایم\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mاید\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mاند\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mبودم\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mبودی\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mبود\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mبودیم\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mبودید\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mبودند\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mباشم\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mباشی\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mباشد\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mباشیم\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mباشید\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mباشند\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     75\u001b[0m \t\t\u001b[39m'\u001b[39m\u001b[39mشده_ام\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mشده_ای\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mشده_است\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mشده_ایم\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mشده_اید\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mشده_اند\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mشده_بودم\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mشده_بودی\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mشده_بود\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mشده_بودیم\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mشده_بودید\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mشده_بودند\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mشده_باشم\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mشده_باشی\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mشده_باشد\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mشده_باشیم\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mشده_باشید\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mشده_باشند\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m \t\t\u001b[39m'\u001b[39m\u001b[39mنخواهم_شد\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mنخواهی_شد\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mنخواهد_شد\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mنخواهیم_شد\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mنخواهید_شد\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mنخواهند_شد\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     83\u001b[0m \t])\n",
      "File \u001b[0;32m~/nlp/hazm-venv/lib/python3.10/site-packages/hazm/utils.py:22\u001b[0m, in \u001b[0;36mwords_list\u001b[0;34m(words_file)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwords_list\u001b[39m(words_file\u001b[39m=\u001b[39mdefault_words):\n\u001b[1;32m     21\u001b[0m \t\u001b[39mwith\u001b[39;00m codecs\u001b[39m.\u001b[39mopen(words_file, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m words_file:\n\u001b[0;32m---> 22\u001b[0m \t\titems \u001b[39m=\u001b[39m [line\u001b[39m.\u001b[39mstrip()\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m'\u001b[39m) \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m words_file]\n\u001b[1;32m     23\u001b[0m \t\t\u001b[39mreturn\u001b[39;00m [(item[\u001b[39m0\u001b[39m], \u001b[39mint\u001b[39m(item[\u001b[39m1\u001b[39m]), \u001b[39mtuple\u001b[39m(item[\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m))) \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m items \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(item) \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m]\n",
      "File \u001b[0;32m~/nlp/hazm-venv/lib/python3.10/site-packages/hazm/utils.py:22\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwords_list\u001b[39m(words_file\u001b[39m=\u001b[39mdefault_words):\n\u001b[1;32m     21\u001b[0m \t\u001b[39mwith\u001b[39;00m codecs\u001b[39m.\u001b[39mopen(words_file, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m words_file:\n\u001b[0;32m---> 22\u001b[0m \t\titems \u001b[39m=\u001b[39m [line\u001b[39m.\u001b[39mstrip()\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m'\u001b[39m) \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m words_file]\n\u001b[1;32m     23\u001b[0m \t\t\u001b[39mreturn\u001b[39;00m [(item[\u001b[39m0\u001b[39m], \u001b[39mint\u001b[39m(item[\u001b[39m1\u001b[39m]), \u001b[39mtuple\u001b[39m(item[\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m))) \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m items \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(item) \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m]\n",
      "File \u001b[0;32m/usr/lib/python3.10/codecs.py:714\u001b[0m, in \u001b[0;36mStreamReaderWriter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    713\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" Return the next decoded line from the input stream.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreader)\n",
      "File \u001b[0;32m/usr/lib/python3.10/codecs.py:645\u001b[0m, in \u001b[0;36mStreamReader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    644\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" Return the next decoded line from the input stream.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 645\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreadline()\n\u001b[1;32m    646\u001b[0m     \u001b[39mif\u001b[39;00m line:\n\u001b[1;32m    647\u001b[0m         \u001b[39mreturn\u001b[39;00m line\n",
      "File \u001b[0;32m/usr/lib/python3.10/codecs.py:531\u001b[0m, in \u001b[0;36mStreamReader.readline\u001b[0;34m(self, size, keepends)\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcharbuffer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcharbuffer[chars:]\n\u001b[1;32m    529\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[0;32m--> 531\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreadline\u001b[39m(\u001b[39mself\u001b[39m, size\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepends\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    533\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" Read one line from the input stream and return the\u001b[39;00m\n\u001b[1;32m    534\u001b[0m \u001b[39m        decoded data.\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    538\u001b[0m \n\u001b[1;32m    539\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    540\u001b[0m     \u001b[39m# If we have lines cached from an earlier read, return\u001b[39;00m\n\u001b[1;32m    541\u001b[0m     \u001b[39m# them unconditionally\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# apply the Normalization and Tokenization to Dataset\n",
    "\n",
    "import hazm as hz\n",
    "\n",
    "comments = dataset[\"comment\"].to_list()\n",
    "for comment in comments:\n",
    "    comment = hz.Normalizer().normalize(comment)\n",
    "    comment = hz.WordTokenizer().tokenize(comment)\n",
    "dataset[\"comment\"] = comments\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  14000\n",
      "Test Data size:  1400\n",
      "Train Data size:  12600\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.sample(frac=1, random_state=4)\n",
    "train_data = dataset[:int(len(dataset)*0.9)]\n",
    "test_data = dataset[int(len(dataset)*0.9):]\n",
    "print(\"Dataset size: \", len(dataset))\n",
    "print(\"Test Data size: \", len(test_data))\n",
    "print(\"Train Data size: \", len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hazm-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
