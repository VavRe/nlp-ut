{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from hazm import Lemmatizer, Normalizer, word_tokenize, stopwords_list\n",
    "import pandas as pd\n",
    "import hazm as hz\n",
    "import pickle\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>واقعا حیف وقت که بنویسم سرویس دهیتون شده افتضاح</td>\n",
       "      <td>SAD</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>قرار بود ۱ ساعته برسه ولی نیم ساعت زودتر از مو...</td>\n",
       "      <td>HAPPY</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>قیمت این مدل اصلا با کیفیتش سازگاری نداره، فقط...</td>\n",
       "      <td>SAD</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>عالللی بود همه چه درست و به اندازه و کیفیت خوب...</td>\n",
       "      <td>HAPPY</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>شیرینی وانیلی فقط یک مدل بود.</td>\n",
       "      <td>HAPPY</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>سلام من به فاکتور غذاهایی که سفارش میدم احتیاج...</td>\n",
       "      <td>SAD</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>سایز پیتزا نسبت به سفارشاتی که قبلا گذشتم کم ش...</td>\n",
       "      <td>SAD</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>من قارچ اضافه رو اضافه کرده بودم بودم اما اگر ...</td>\n",
       "      <td>HAPPY</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>همرو بعد ۲ساعت تاخیر اشتباه آوردن پولشم رفت رو...</td>\n",
       "      <td>SAD</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>فلفلش خییییلی تند بود.</td>\n",
       "      <td>HAPPY</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 comment  label  label_id\n",
       "0        واقعا حیف وقت که بنویسم سرویس دهیتون شده افتضاح    SAD         1\n",
       "1      قرار بود ۱ ساعته برسه ولی نیم ساعت زودتر از مو...  HAPPY         0\n",
       "2      قیمت این مدل اصلا با کیفیتش سازگاری نداره، فقط...    SAD         1\n",
       "3      عالللی بود همه چه درست و به اندازه و کیفیت خوب...  HAPPY         0\n",
       "4                          شیرینی وانیلی فقط یک مدل بود.  HAPPY         0\n",
       "...                                                  ...    ...       ...\n",
       "69995  سلام من به فاکتور غذاهایی که سفارش میدم احتیاج...    SAD         1\n",
       "69996  سایز پیتزا نسبت به سفارشاتی که قبلا گذشتم کم ش...    SAD         1\n",
       "69997  من قارچ اضافه رو اضافه کرده بودم بودم اما اگر ...  HAPPY         0\n",
       "69998  همرو بعد ۲ساعت تاخیر اشتباه آوردن پولشم رفت رو...    SAD         1\n",
       "69999                             فلفلش خییییلی تند بود.  HAPPY         0\n",
       "\n",
       "[70000 rows x 3 columns]"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('./datasets/Snappfood - Sentiment Analysis.csv', delimiter=\"\\t\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SAD      35000\n",
       "HAPPY    35000\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HAPPY    7000\n",
       "SAD      7000\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there is two class of SAD and HAPPY in dataset, take 20% of data with the same class ditribution\n",
    "dataset = pd.concat([dataset[dataset['label'] == 'HAPPY'].sample(frac=0.2, random_state=97), dataset[dataset['label'] == 'SAD'].sample(frac=0.2, random_state=97)]).reset_index()\n",
    "dataset[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.iloc[-1][\"comment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_pipieline(sentence: List[str] | str ,stopwords: List[str] ,lemma: Lemmatizer,normalizer:Normalizer,index: int, dataset_len: int) -> List[str]:\n",
    "    sentence = normalizer.normalize(sentence)\n",
    "    sentence = hz.word_tokenize(sentence)\n",
    "    acc = 0\n",
    "    \n",
    "    for word in sentence:\n",
    "        if word in stopwords:   \n",
    "            sentence.remove(word)\n",
    "        word = lemma.lemmatize(word)\n",
    "    percentage = 100*(index+1)/dataset_len\n",
    "    if percentage % 5 == 0:\n",
    "        print(f\"Processing iteration {index+1}/{dataset_len} ({percentage:.0f}%)\")\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing iteration 700/14000 (5%)\n",
      "Processing iteration 1400/14000 (10%)\n",
      "Processing iteration 2100/14000 (15%)\n",
      "Processing iteration 2800/14000 (20%)\n",
      "Processing iteration 3500/14000 (25%)\n",
      "Processing iteration 4200/14000 (30%)\n",
      "Processing iteration 4900/14000 (35%)\n",
      "Processing iteration 5600/14000 (40%)\n",
      "Processing iteration 6300/14000 (45%)\n",
      "Processing iteration 7000/14000 (50%)\n",
      "Processing iteration 7700/14000 (55%)\n",
      "Processing iteration 8400/14000 (60%)\n",
      "Processing iteration 9100/14000 (65%)\n",
      "Processing iteration 9800/14000 (70%)\n",
      "Processing iteration 10500/14000 (75%)\n",
      "Processing iteration 11200/14000 (80%)\n",
      "Processing iteration 11900/14000 (85%)\n",
      "Processing iteration 12600/14000 (90%)\n",
      "Processing iteration 13300/14000 (95%)\n",
      "Processing iteration 14000/14000 (100%)\n"
     ]
    }
   ],
   "source": [
    "stopwords = hz.stopwords_list()\n",
    "lemmatizer = hz.Lemmatizer()\n",
    "normalizer = hz.Normalizer()\n",
    "dataset[\"comment\"] = dataset.apply(lambda row: preprocessing_pipieline(row[\"comment\"],stopwords,lemmatizer,normalizer,row.name,len(dataset)),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45776</td>\n",
       "      <td>[همبرگر, خوشمزه, و, پیتزا, کیفیتش, بود]</td>\n",
       "      <td>HAPPY</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8003</td>\n",
       "      <td>[خوب, فقط, سس, فراموش]</td>\n",
       "      <td>HAPPY</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3124</td>\n",
       "      <td>[عالی, واقعا, ممنونم, امیدوارم, همینطوری, بمونه]</td>\n",
       "      <td>HAPPY</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27801</td>\n",
       "      <td>[همیشه, ., …فقط, سیب, زمینی‌های, اتون, معمولی,...</td>\n",
       "      <td>HAPPY</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26997</td>\n",
       "      <td>[دقیقاااااا, سر, موقع, ۳۰, ثانیه, زودتر, شیرین...</td>\n",
       "      <td>HAPPY</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13995</th>\n",
       "      <td>35802</td>\n",
       "      <td>[رول, پای, سیب, بد, ., میکادو, کلا, خورد, تو, ...</td>\n",
       "      <td>SAD</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13996</th>\n",
       "      <td>15602</td>\n",
       "      <td>[مغز, کنافه‌ها, نشاسته, خمیر, نه, سرشیر, ،, شی...</td>\n",
       "      <td>SAD</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13997</th>\n",
       "      <td>55717</td>\n",
       "      <td>[سلام, سفارشم, ۱۲, عدد, سنگگ, معمولی, هزیته, پ...</td>\n",
       "      <td>SAD</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13998</th>\n",
       "      <td>16481</td>\n",
       "      <td>[افتضاح‌ترین, سوپ, قارچ, افتضاح]</td>\n",
       "      <td>SAD</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13999</th>\n",
       "      <td>60085</td>\n",
       "      <td>[ساندویچ, اونی, داخل, عکس, ونوشته, کیفیت, نداش...</td>\n",
       "      <td>SAD</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                                            comment  label  \\\n",
       "0      45776            [همبرگر, خوشمزه, و, پیتزا, کیفیتش, بود]  HAPPY   \n",
       "1       8003                             [خوب, فقط, سس, فراموش]  HAPPY   \n",
       "2       3124   [عالی, واقعا, ممنونم, امیدوارم, همینطوری, بمونه]  HAPPY   \n",
       "3      27801  [همیشه, ., …فقط, سیب, زمینی‌های, اتون, معمولی,...  HAPPY   \n",
       "4      26997  [دقیقاااااا, سر, موقع, ۳۰, ثانیه, زودتر, شیرین...  HAPPY   \n",
       "...      ...                                                ...    ...   \n",
       "13995  35802  [رول, پای, سیب, بد, ., میکادو, کلا, خورد, تو, ...    SAD   \n",
       "13996  15602  [مغز, کنافه‌ها, نشاسته, خمیر, نه, سرشیر, ،, شی...    SAD   \n",
       "13997  55717  [سلام, سفارشم, ۱۲, عدد, سنگگ, معمولی, هزیته, پ...    SAD   \n",
       "13998  16481                   [افتضاح‌ترین, سوپ, قارچ, افتضاح]    SAD   \n",
       "13999  60085  [ساندویچ, اونی, داخل, عکس, ونوشته, کیفیت, نداش...    SAD   \n",
       "\n",
       "       label_id  \n",
       "0             0  \n",
       "1             0  \n",
       "2             0  \n",
       "3             0  \n",
       "4             0  \n",
       "...         ...  \n",
       "13995         1  \n",
       "13996         1  \n",
       "13997         1  \n",
       "13998         1  \n",
       "13999         1  \n",
       "\n",
       "[14000 rows x 4 columns]"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('./datasets/preprocessed.pkl', 'wb')\n",
    "with open('./datasets/preprocessed.pkl', 'wb') as f:\n",
    "    pickle.dump(dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "for word in hz.stopwords_list():\n",
    "    if word == \"تنها\":\n",
    "        print(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/vavre/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "No such file or directory: '/home/vavre/nltk_data/corpora/stopwords/persian'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[309], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m nltk\u001b[39m.\u001b[39mdownload(\u001b[39m'\u001b[39m\u001b[39mstopwords\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m stopwords \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(nltk\u001b[39m.\u001b[39;49mcorpus\u001b[39m.\u001b[39;49mstopwords\u001b[39m.\u001b[39;49mwords(\u001b[39m'\u001b[39;49m\u001b[39mpersian\u001b[39;49m\u001b[39m'\u001b[39;49m))\n",
      "File \u001b[0;32m~/nlp/hazm-venv/lib/python3.10/site-packages/nltk/corpus/reader/wordlist.py:22\u001b[0m, in \u001b[0;36mWordListCorpusReader.words\u001b[0;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwords\u001b[39m(\u001b[39mself\u001b[39m, fileids\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, ignore_lines_startswith\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m):\n\u001b[0;32m---> 22\u001b[0m     \u001b[39mreturn\u001b[39;00m [line \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m line_tokenize(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw(fileids))\n\u001b[1;32m     23\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m line\u001b[39m.\u001b[39mstartswith(ignore_lines_startswith)]\n",
      "File \u001b[0;32m~/nlp/hazm-venv/lib/python3.10/site-packages/nltk/corpus/reader/wordlist.py:28\u001b[0m, in \u001b[0;36mWordListCorpusReader.raw\u001b[0;34m(self, fileids)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mif\u001b[39;00m fileids \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m: fileids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fileids\n\u001b[1;32m     27\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(fileids, string_types): fileids \u001b[39m=\u001b[39m [fileids]\n\u001b[0;32m---> 28\u001b[0m \u001b[39mreturn\u001b[39;00m concat([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopen(f)\u001b[39m.\u001b[39mread() \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m fileids])\n",
      "File \u001b[0;32m~/nlp/hazm-venv/lib/python3.10/site-packages/nltk/corpus/reader/wordlist.py:28\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mif\u001b[39;00m fileids \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m: fileids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fileids\n\u001b[1;32m     27\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(fileids, string_types): fileids \u001b[39m=\u001b[39m [fileids]\n\u001b[0;32m---> 28\u001b[0m \u001b[39mreturn\u001b[39;00m concat([\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mopen(f)\u001b[39m.\u001b[39mread() \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m fileids])\n",
      "File \u001b[0;32m~/nlp/hazm-venv/lib/python3.10/site-packages/nltk/corpus/reader/api.py:213\u001b[0m, in \u001b[0;36mCorpusReader.open\u001b[0;34m(self, file)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[39mReturn an open stream that can be used to read the given file.\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39mIf the file's encoding is not None, then the stream will\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39m:param file: The file identifier of the file to read.\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    212\u001b[0m encoding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoding(file)\n\u001b[0;32m--> 213\u001b[0m stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_root\u001b[39m.\u001b[39;49mjoin(file)\u001b[39m.\u001b[39mopen(encoding)\n\u001b[1;32m    214\u001b[0m \u001b[39mreturn\u001b[39;00m stream\n",
      "File \u001b[0;32m~/nlp/hazm-venv/lib/python3.10/site-packages/nltk/data.py:340\u001b[0m, in \u001b[0;36mFileSystemPathPointer.join\u001b[0;34m(self, fileid)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mjoin\u001b[39m(\u001b[39mself\u001b[39m, fileid):\n\u001b[1;32m    339\u001b[0m     _path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_path, fileid)\n\u001b[0;32m--> 340\u001b[0m     \u001b[39mreturn\u001b[39;00m FileSystemPathPointer(_path)\n",
      "File \u001b[0;32m~/nlp/hazm-venv/lib/python3.10/site-packages/nltk/compat.py:221\u001b[0m, in \u001b[0;36mpy3_data.<locals>._decorator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_decorator\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    220\u001b[0m     args \u001b[39m=\u001b[39m (args[\u001b[39m0\u001b[39m], add_py3_data(args[\u001b[39m1\u001b[39m])) \u001b[39m+\u001b[39m args[\u001b[39m2\u001b[39m:]\n\u001b[0;32m--> 221\u001b[0m     \u001b[39mreturn\u001b[39;00m init_func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/nlp/hazm-venv/lib/python3.10/site-packages/nltk/data.py:318\u001b[0m, in \u001b[0;36mFileSystemPathPointer.__init__\u001b[0;34m(self, _path)\u001b[0m\n\u001b[1;32m    316\u001b[0m _path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mabspath(_path)\n\u001b[1;32m    317\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(_path):\n\u001b[0;32m--> 318\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mNo such file or directory: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m _path)\n\u001b[1;32m    319\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_path \u001b[39m=\u001b[39m _path\n",
      "\u001b[0;31mOSError\u001b[0m: No such file or directory: '/home/vavre/nltk_data/corpora/stopwords/persian'"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stopwords = set(nltk.corpus.stopwords.words('persian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing iteration 250/5000 (5%)\n",
      "Processing iteration 500/5000 (10%)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[141], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m random_list_of_numbers \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m,\u001b[39m100\u001b[39m,\u001b[39m5000\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m index, number \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(random_list_of_numbers):\n\u001b[1;32m      7\u001b[0m     \u001b[39m# pause thread for 0.01 seconds\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.05\u001b[39;49m)\n\u001b[1;32m      9\u001b[0m     percentage \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m\u001b[39m*\u001b[39m(index\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m/\u001b[39m\u001b[39m5000\u001b[39m\n\u001b[1;32m     10\u001b[0m     \u001b[39mif\u001b[39;00m percentage \u001b[39m%\u001b[39m \u001b[39m5\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "random_list_of_numbers = np.random.randint(0,100,5000)\n",
    "\n",
    "for index, number in enumerate(random_list_of_numbers):\n",
    "    # pause thread for 0.01 seconds\n",
    "    time.sleep(0.05)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hazm-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
