{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from hazm import Lemmatizer, Normalizer, word_tokenize, stopwords_list\n",
    "import pandas as pd\n",
    "import hazm as hz\n",
    "import pickle\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>واقعا حیف وقت که بنویسم سرویس دهیتون شده افتضاح</td>\n",
       "      <td>SAD</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>قرار بود ۱ ساعته برسه ولی نیم ساعت زودتر از مو...</td>\n",
       "      <td>HAPPY</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>قیمت این مدل اصلا با کیفیتش سازگاری نداره، فقط...</td>\n",
       "      <td>SAD</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>عالللی بود همه چه درست و به اندازه و کیفیت خوب...</td>\n",
       "      <td>HAPPY</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>شیرینی وانیلی فقط یک مدل بود.</td>\n",
       "      <td>HAPPY</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>سلام من به فاکتور غذاهایی که سفارش میدم احتیاج...</td>\n",
       "      <td>SAD</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>سایز پیتزا نسبت به سفارشاتی که قبلا گذشتم کم ش...</td>\n",
       "      <td>SAD</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>من قارچ اضافه رو اضافه کرده بودم بودم اما اگر ...</td>\n",
       "      <td>HAPPY</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>همرو بعد ۲ساعت تاخیر اشتباه آوردن پولشم رفت رو...</td>\n",
       "      <td>SAD</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>فلفلش خییییلی تند بود.</td>\n",
       "      <td>HAPPY</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 comment  label  label_id\n",
       "0        واقعا حیف وقت که بنویسم سرویس دهیتون شده افتضاح    SAD         1\n",
       "1      قرار بود ۱ ساعته برسه ولی نیم ساعت زودتر از مو...  HAPPY         0\n",
       "2      قیمت این مدل اصلا با کیفیتش سازگاری نداره، فقط...    SAD         1\n",
       "3      عالللی بود همه چه درست و به اندازه و کیفیت خوب...  HAPPY         0\n",
       "4                          شیرینی وانیلی فقط یک مدل بود.  HAPPY         0\n",
       "...                                                  ...    ...       ...\n",
       "69995  سلام من به فاکتور غذاهایی که سفارش میدم احتیاج...    SAD         1\n",
       "69996  سایز پیتزا نسبت به سفارشاتی که قبلا گذشتم کم ش...    SAD         1\n",
       "69997  من قارچ اضافه رو اضافه کرده بودم بودم اما اگر ...  HAPPY         0\n",
       "69998  همرو بعد ۲ساعت تاخیر اشتباه آوردن پولشم رفت رو...    SAD         1\n",
       "69999                             فلفلش خییییلی تند بود.  HAPPY         0\n",
       "\n",
       "[70000 rows x 3 columns]"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('./datasets/Snappfood - Sentiment Analysis.csv', delimiter=\"\\t\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SAD      35000\n",
       "HAPPY    35000\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HAPPY    7000\n",
       "SAD      7000\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there is two class of SAD and HAPPY in dataset, take 20% of data with the same class ditribution\n",
    "dataset = pd.concat([dataset[dataset['label'] == 'HAPPY'].sample(frac=0.2, random_state=97), dataset[dataset['label'] == 'SAD'].sample(frac=0.2, random_state=97)]).reset_index()\n",
    "dataset[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.iloc[-1][\"comment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocessing_pipieline(sentence ,stopwords: List[str] ,lemma: Lemmatizer,normalizer:Normalizer,index: int, dataset_len: int) -> List[str]:\n",
    "    sentence = normalizer.normalize(sentence)\n",
    "    sentence = hz.word_tokenize(sentence)\n",
    "    acc = 0\n",
    "    #remove all non-persian characters\n",
    "\n",
    "    for word in sentence:\n",
    "        if word in stopwords:   \n",
    "            sentence.remove(word)\n",
    "        if re.match(r'^[a-zA-Z0-9_]+$', word):\n",
    "            sentence.remove(word)\n",
    "        word = lemma.lemmatize(word)\n",
    "    percentage = 100*(index+1)/dataset_len\n",
    "    if percentage % 5 == 0:\n",
    "        print(f\"Processing iteration {index+1}/{dataset_len} ({percentage:.0f}%)\")\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing iteration 700/14000 (5%)\n",
      "Processing iteration 1400/14000 (10%)\n",
      "Processing iteration 2100/14000 (15%)\n",
      "Processing iteration 2800/14000 (20%)\n",
      "Processing iteration 3500/14000 (25%)\n",
      "Processing iteration 4200/14000 (30%)\n",
      "Processing iteration 4900/14000 (35%)\n",
      "Processing iteration 5600/14000 (40%)\n",
      "Processing iteration 6300/14000 (45%)\n",
      "Processing iteration 7000/14000 (50%)\n",
      "Processing iteration 7700/14000 (55%)\n",
      "Processing iteration 8400/14000 (60%)\n",
      "Processing iteration 9100/14000 (65%)\n",
      "Processing iteration 9800/14000 (70%)\n",
      "Processing iteration 10500/14000 (75%)\n",
      "Processing iteration 11200/14000 (80%)\n",
      "Processing iteration 11900/14000 (85%)\n",
      "Processing iteration 12600/14000 (90%)\n",
      "Processing iteration 13300/14000 (95%)\n",
      "Processing iteration 14000/14000 (100%)\n"
     ]
    }
   ],
   "source": [
    "stopwords = hz.stopwords_list()\n",
    "lemmatizer = hz.Lemmatizer()\n",
    "normalizer = hz.Normalizer(affix_spacing=False)\n",
    "dataset[\"comment\"] = dataset.apply(lambda row: preprocessing_pipieline(row[\"comment\"],stopwords,lemmatizer,normalizer,row.name,len(dataset)),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('./datasets/preprocessed.pkl', 'wb')\n",
    "with open('./datasets/preprocessed.pkl', 'wb') as f:\n",
    "    pickle.dump(dataset, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hazm-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
