{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/VavRe/nlp-ut/blob/main/CA2/Q3.ipynb",
      "authorship_tag": "ABX9TyM3Tj5o9WTIQsmGPOSVPpwl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VavRe/nlp-ut/blob/main/CA2/Q3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5ye8TmiMlrfd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb57d6c8-eb31-4544-8bfd-c00297cf623c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.13)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.65.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.27.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2022.12.7)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.26.15)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4)\n",
            "mkdir: cannot create directory ‘/root/.kaggle/’: File exists\n",
            "sentiment-analysis-for-financial-news.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Archive:  /content/sentiment-analysis-for-financial-news.zip\n",
            "replace FinancialPhraseBank/License.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle\n",
        "!mkdir ~/.kaggle/\n",
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/ ;\n",
        "!chmod 600 ~/.kaggle/kaggle.json ; \n",
        "!kaggle datasets download -d ankurzing/sentiment-analysis-for-financial-news ;\n",
        "!unzip /content/sentiment-analysis-for-financial-news.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "id": "4E_rZW2k9DSo",
        "outputId": "bbc5a0d2-5432-4cea-f2e4-45fd5dda4ac8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv(\"./all-data.csv\", encoding='iso-8859-1', header=None)\n",
        "dataset.columns = [\"sentiment\",\"phrase\"]"
      ],
      "metadata": {
        "id": "QwefyjFS_rO5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"sentiment\"].value_counts()"
      ],
      "metadata": {
        "id": "PZGcH3P0LiXe",
        "outputId": "ba4fd7b1-7364-4fc9-a595-7f1651ed0056",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "neutral     2879\n",
              "positive    1363\n",
              "negative     604\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# def preprocess(text):\n",
        "#     tokens = word_tokenize(text.lower())\n",
        "#     filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "#     lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
        "#     preprocessed_text = ' '.join(lemmatized_tokens)\n",
        "#     return preprocessed_text"
      ],
      "metadata": {
        "id": "bt-_KeYCbJdY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "def preprocess_text(text):\n",
        "    # tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "    \n",
        "    # initialize a list to store the preprocessed tokens\n",
        "    preprocessed_tokens = []\n",
        "    \n",
        "    # iterate over the tokens\n",
        "    i = 0\n",
        "    while i < len(tokens):\n",
        "        token = tokens[i]\n",
        "        \n",
        "        # check if the token is a stop word or a punctuation\n",
        "        if token.lower() not in stop_words and token not in string.punctuation:\n",
        "            # lemmatize the token\n",
        "            lemma = lemmatizer.lemmatize(token.lower())\n",
        "            \n",
        "            # check if the token is negative\n",
        "            if i < len(tokens) - 1 and tokens[i+1] in ['not', \"n't\"]:\n",
        "                preprocessed_tokens.append('NOT_' + lemma)\n",
        "                i += 1\n",
        "            elif lemma in ['negative', 'bad', 'poor', 'terrible', 'horrible']:\n",
        "                preprocessed_tokens.append('NOT_' + lemma)\n",
        "            else:\n",
        "                preprocessed_tokens.append(lemma)\n",
        "        \n",
        "        i += 1\n",
        "    \n",
        "    return preprocessed_tokens"
      ],
      "metadata": {
        "id": "JR-cgbovrFLh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = LabelEncoder()\n",
        "dataset[\"label\"] = encoder.fit_transform(dataset[\"sentiment\"])\n",
        "dataset"
      ],
      "metadata": {
        "id": "RABMhC9_tokz",
        "outputId": "9a6f2f29-142d-434d-bbac-c0ae2dac9b90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     sentiment                                             phrase  label\n",
              "0      neutral  According to Gran , the company has no plans t...      1\n",
              "1      neutral  Technopolis plans to develop in stages an area...      1\n",
              "2     negative  The international electronic industry company ...      0\n",
              "3     positive  With the new production plant the company woul...      2\n",
              "4     positive  According to the company 's updated strategy f...      2\n",
              "...        ...                                                ...    ...\n",
              "4841  negative  LONDON MarketWatch -- Share prices ended lower...      0\n",
              "4842   neutral  Rinkuskiai 's beer sales fell by 6.5 per cent ...      1\n",
              "4843  negative  Operating profit fell to EUR 35.4 mn from EUR ...      0\n",
              "4844  negative  Net sales of the Paper segment decreased to EU...      0\n",
              "4845  negative  Sales in Finland decreased by 10.5 % in Januar...      0\n",
              "\n",
              "[4846 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c0bbbc42-c4ea-4197-8e17-b1b7316d504b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>phrase</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>neutral</td>\n",
              "      <td>According to Gran , the company has no plans t...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>neutral</td>\n",
              "      <td>Technopolis plans to develop in stages an area...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>negative</td>\n",
              "      <td>The international electronic industry company ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>positive</td>\n",
              "      <td>With the new production plant the company woul...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>positive</td>\n",
              "      <td>According to the company 's updated strategy f...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4841</th>\n",
              "      <td>negative</td>\n",
              "      <td>LONDON MarketWatch -- Share prices ended lower...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4842</th>\n",
              "      <td>neutral</td>\n",
              "      <td>Rinkuskiai 's beer sales fell by 6.5 per cent ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4843</th>\n",
              "      <td>negative</td>\n",
              "      <td>Operating profit fell to EUR 35.4 mn from EUR ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4844</th>\n",
              "      <td>negative</td>\n",
              "      <td>Net sales of the Paper segment decreased to EU...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4845</th>\n",
              "      <td>negative</td>\n",
              "      <td>Sales in Finland decreased by 10.5 % in Januar...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4846 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c0bbbc42-c4ea-4197-8e17-b1b7316d504b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c0bbbc42-c4ea-4197-8e17-b1b7316d504b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c0bbbc42-c4ea-4197-8e17-b1b7316d504b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"phrase\"] = dataset[\"phrase\"].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "nC5i2-qbv-BM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"phrase\"]"
      ],
      "metadata": {
        "id": "guNkhfXZwcm4",
        "outputId": "53e61e3c-7b04-42ab-a9ae-a687e87c6219",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       [according, gran, company, plan, move, product...\n",
              "1       [technopolis, plan, develop, stage, area, le, ...\n",
              "2       [international, electronic, industry, company,...\n",
              "3       [new, production, plant, company, would, incre...\n",
              "4       [according, company, 's, updated, strategy, ye...\n",
              "                              ...                        \n",
              "4841    [london, marketwatch, --, share, price, ended,...\n",
              "4842    [rinkuskiai, 's, beer, sale, fell, 6.5, per, c...\n",
              "4843    [operating, profit, fell, eur, 35.4, mn, eur, ...\n",
              "4844    [net, sale, paper, segment, decreased, eur, 22...\n",
              "4845    [sale, finland, decreased, 10.5, january, sale...\n",
              "Name: phrase, Length: 4846, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "# !unzip glove.6B.zip -d glove.6B\n"
      ],
      "metadata": {
        "id": "WGGsUfLE0Ihy",
        "outputId": "0a8d928b-90f5-491d-8ca0-dc741e9252d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-06 14:37:46--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2023-05-06 14:37:46--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2023-05-06 14:37:47--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.2’\n",
            "\n",
            "glove.6B.zip.2      100%[===================>] 822.24M  5.01MB/s    in 2m 39s  \n",
            "\n",
            "2023-05-06 14:40:26 (5.17 MB/s) - ‘glove.6B.zip.2’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "replace glove.6B/glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace glove.6B/glove.6B.100d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "embeddings_index = {}\n",
        "with open('glove.6B/glove.6B.100d.txt', 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        embeddings = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = embeddings\n"
      ],
      "metadata": {
        "id": "6wQFZW4U1dH4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(dataset['phrase'])\n",
        "\n",
        "phrases = tokenizer.texts_to_sequences(dataset['phrase'])\n",
        "labels = dataset['label']\n",
        "\n",
        "from keras.utils import pad_sequences\n",
        "\n",
        "maxlen = 100\n",
        "X = pad_sequences(phrases, padding='post', maxlen=maxlen)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embedding_dim = 100\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.1, random_state=42, stratify=labels)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "vsDAzJM133c8",
        "outputId": "d993ad32-b2e1-4498-c68a-c65f84b2257a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        61\n",
            "           1       0.59      0.96      0.73       288\n",
            "           2       0.24      0.03      0.05       136\n",
            "\n",
            "    accuracy                           0.58       485\n",
            "   macro avg       0.28      0.33      0.26       485\n",
            "weighted avg       0.42      0.58      0.45       485\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "phrases[0]"
      ],
      "metadata": {
        "id": "hVtFcB9AUQnu",
        "outputId": "30a17a06-72f1-45a7-f175-6088c39078d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[57, 3251, 3, 119, 519, 53, 89, 2555, 3, 631]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IcmSopId-sIS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install zeugma"
      ],
      "metadata": {
        "id": "o9IjuLAh8uDe",
        "outputId": "b69bd735-aebc-4d6d-9249-b54db3e3d4c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: zeugma in /usr/local/lib/python3.10/dist-packages (0.49)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from zeugma) (1.2.2)\n",
            "Requirement already satisfied: gensim>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from zeugma) (4.3.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from zeugma) (1.22.4)\n",
            "Requirement already satisfied: keras>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from zeugma) (2.12.0)\n",
            "Requirement already satisfied: tensorflow>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from zeugma) (2.12.0)\n",
            "Requirement already satisfied: pandas>=0.20.3 in /usr/local/lib/python3.10/dist-packages (from zeugma) (1.5.3)\n",
            "Requirement already satisfied: Cython>=0.27.3 in /usr/local/lib/python3.10/dist-packages (from zeugma) (0.29.34)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim>=3.5.0->zeugma) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim>=3.5.0->zeugma) (6.3.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.20.3->zeugma) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.20.3->zeugma) (2.8.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->zeugma) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->zeugma) (3.1.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.5.0->zeugma) (1.54.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.5.0->zeugma) (2.12.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.5.0->zeugma) (1.14.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.5.0->zeugma) (23.1)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.5.0->zeugma) (23.3.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.5.0->zeugma) (1.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.5.0->zeugma) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.5.0->zeugma) (4.5.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.5.0->zeugma) (0.4.8)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.5.0->zeugma) (3.8.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.5.0->zeugma) (1.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.5.0->zeugma) (0.2.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.5.0->zeugma) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.5.0->zeugma) (3.3.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.5.0->zeugma) (16.0.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.5.0->zeugma) (2.12.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.5.0->zeugma) (2.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.5.0->zeugma) (3.20.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.5.0->zeugma) (0.32.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.5.0->zeugma) (1.6.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=1.5.0->zeugma) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow>=1.5.0->zeugma) (0.1.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=1.5.0->zeugma) (2.3.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=1.5.0->zeugma) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=1.5.0->zeugma) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=1.5.0->zeugma) (3.4.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=1.5.0->zeugma) (2.17.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=1.5.0->zeugma) (0.7.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=1.5.0->zeugma) (1.0.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=1.5.0->zeugma) (0.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=1.5.0->zeugma) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=1.5.0->zeugma) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=1.5.0->zeugma) (1.3.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow>=1.5.0->zeugma) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow>=1.5.0->zeugma) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow>=1.5.0->zeugma) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow>=1.5.0->zeugma) (2.0.12)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow>=1.5.0->zeugma) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=1.5.0->zeugma) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=1.5.0->zeugma) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(dataset[\"phrase\"].to_list(), dataset[\"label\"].to_list(), test_size=0.1, stratify=dataset[\"label\"].to_list(), random_state=42)\n"
      ],
      "metadata": {
        "id": "0PPq7mJ79Sog"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from zeugma.embeddings import EmbeddingTransformer\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "glove = EmbeddingTransformer('glove')\n",
        "X_train = glove.transform(X_train)\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "X_test = glove.transform(X_test)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "target_names = ['Negative', 'Neutral', 'Positive']\n",
        "\n",
        "print(classification_report(y_test, y_pred, target_names=target_names))\n"
      ],
      "metadata": {
        "id": "AO3i_EvS8slH",
        "outputId": "5254e0fc-c084-4551-b3b9-c8d2ba64164a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-53781840dd99>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mglove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbeddingTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'glove'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglove\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/zeugma/embeddings.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;34m\"\"\"Transform corpus from single text transformation method\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;31m# TODO: parallelize this method with multiprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/zeugma/embeddings.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;34m\"\"\"Transform corpus from single text transformation method\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;31m# TODO: parallelize this method with multiprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/zeugma/embeddings.py\u001b[0m in \u001b[0;36mtransform_sentence\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/zeugma/embeddings.py\u001b[0m in \u001b[0;36mpreprocess_text\u001b[0;34m(raw_text)\u001b[0m\n\u001b[1;32m     73\u001b[0m                     )\n\u001b[1;32m     74\u001b[0m                 \u001b[0mraw_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'raw_tokens' referenced before assignment"
          ]
        }
      ]
    }
  ]
}