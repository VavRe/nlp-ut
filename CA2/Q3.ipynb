{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1eHfg8DIiWdZMmQbqBm0Sc1fSEeBGUNii",
      "authorship_tag": "ABX9TyPEyHj2KBuiqnhWstfIHtbB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VavRe/nlp-ut/blob/main/CA2/Q3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "5ye8TmiMlrfd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05f1637d-9326-4bfe-8c17-53269a4d58db"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.13)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2022.12.7)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.26.15)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.65.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (2.0.12)\n",
            "mkdir: cannot create directory ‘/root/.kaggle/’: File exists\n",
            "sentiment-analysis-for-financial-news.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Archive:  /content/sentiment-analysis-for-financial-news.zip\n",
            "replace FinancialPhraseBank/License.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "!pip install kaggle\n",
        "!mkdir ~/.kaggle/\n",
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/ ;\n",
        "!chmod 600 ~/.kaggle/kaggle.json ; \n",
        "!kaggle datasets download -d ankurzing/sentiment-analysis-for-financial-news ;\n",
        "!unzip /content/sentiment-analysis-for-financial-news.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "id": "4E_rZW2k9DSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv(\"./all-data.csv\", encoding='iso-8859-1', header=None)\n",
        "dataset.columns = [\"sentiment\",\"phrase\"]"
      ],
      "metadata": {
        "id": "QwefyjFS_rO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"sentiment\"].value_counts()"
      ],
      "metadata": {
        "id": "PZGcH3P0LiXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# def preprocess(text):\n",
        "#     tokens = word_tokenize(text.lower())\n",
        "#     filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "#     lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
        "#     preprocessed_text = ' '.join(lemmatized_tokens)\n",
        "#     return preprocessed_text"
      ],
      "metadata": {
        "id": "bt-_KeYCbJdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "    \n",
        "    # initialize a list to store the preprocessed tokens\n",
        "    preprocessed_tokens = []\n",
        "    \n",
        "    # iterate over the tokens\n",
        "    i = 0\n",
        "    while i < len(tokens):\n",
        "        token = tokens[i]\n",
        "        \n",
        "        # check if the token is a stop word or a punctuation\n",
        "        if token.lower() not in stop_words and token not in punctuations:\n",
        "            # lemmatize the token\n",
        "            lemma = lemmatizer.lemmatize(token.lower())\n",
        "            \n",
        "            # check if the token is negative\n",
        "            if i < len(tokens) - 1 and tokens[i+1] in ['not', \"n't\"]:\n",
        "                preprocessed_tokens.append('NOT_' + lemma)\n",
        "                i += 1\n",
        "            elif lemma in ['negative', 'bad', 'poor', 'terrible', 'horrible']:\n",
        "                preprocessed_tokens.append('NOT_' + lemma)\n",
        "            else:\n",
        "                preprocessed_tokens.append(lemma)\n",
        "        \n",
        "        i += 1\n",
        "    \n",
        "    return preprocessed_tokens"
      ],
      "metadata": {
        "id": "JR-cgbovrFLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = LabelEncoder()\n",
        "dataset[\"label\"] = encoder.fit_transform(dataset[\"sentiment\"])\n",
        "dataset"
      ],
      "metadata": {
        "id": "RABMhC9_tokz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"phrase\"] = dataset[\"phrase\"].apply(preprocess)"
      ],
      "metadata": {
        "id": "nC5i2-qbv-BM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"phrase\"]"
      ],
      "metadata": {
        "id": "guNkhfXZwcm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip -d glove.6B\n"
      ],
      "metadata": {
        "id": "WGGsUfLE0Ihy",
        "outputId": "a6bfa10f-2da9-43f4-ee09-c78223c2d02c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-06 14:24:04--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2023-05-06 14:24:04--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2023-05-06 14:24:04--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.1’\n",
            "\n",
            "glove.6B.zip.1      100%[===================>] 822.24M  5.00MB/s    in 2m 39s  \n",
            "\n",
            "2023-05-06 14:26:43 (5.17 MB/s) - ‘glove.6B.zip.1’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "replace glove.6B/glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "embeddings_index = {}\n",
        "with open('glove.6B/glove.6B.100d.txt', 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        embeddings = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = embeddings\n"
      ],
      "metadata": {
        "id": "6wQFZW4U1dH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(dataset['phrase'])\n",
        "\n",
        "phrases = tokenizer.texts_to_sequences(dataset['phrase'])\n",
        "labels = dataset['label']\n",
        "\n",
        "from keras.utils import pad_sequences\n",
        "\n",
        "maxlen = 100\n",
        "X = pad_sequences(phrases, padding='post', maxlen=maxlen)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embedding_dim = 100\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.1, random_state=42, stratify=labels)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "vsDAzJM133c8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IcmSopId-sIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install zeugma"
      ],
      "metadata": {
        "id": "o9IjuLAh8uDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(dataset[\"phrase\"].to_list(), dataset[\"label\"].to_list(), test_size=0.1, stratify=dataset[\"label\"].to_list(), random_state=42)\n"
      ],
      "metadata": {
        "id": "0PPq7mJ79Sog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from zeugma.embeddings import EmbeddingTransformer\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "glove = EmbeddingTransformer('glove')\n",
        "X_train = glove.transform(X_train)\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "X_test = glove.transform(X_test)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "target_names = ['Negative', 'Neutral', 'Positive']\n",
        "\n",
        "print(classification_report(y_test, y_pred, target_names=target_names))\n"
      ],
      "metadata": {
        "id": "AO3i_EvS8slH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}