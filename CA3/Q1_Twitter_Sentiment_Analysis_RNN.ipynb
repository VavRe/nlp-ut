{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VavRe/nlp-ut/blob/main/CA3/Q1_Twitter_Sentiment_Analysis_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fI1zESTpRII0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da4beb92-07df-443c-a2de-9e0d01fcde84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "URL transformed to HTTPS due to an HSTS policy\n",
            "--2023-05-13 20:07:45--  https://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\n",
            "Resolving cs.stanford.edu (cs.stanford.edu)... 171.64.64.64\n",
            "Connecting to cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 81363704 (78M) [application/zip]\n",
            "Saving to: ‘trainingandtestdata.zip.2’\n",
            "\n",
            "trainingandtestdata 100%[===================>]  77.59M  96.1MB/s    in 0.8s    \n",
            "\n",
            "2023-05-13 20:07:46 (96.1 MB/s) - ‘trainingandtestdata.zip.2’ saved [81363704/81363704]\n",
            "\n",
            "Archive:  trainingandtestdata.zip\n",
            "replace testdata.manual.2009.06.14.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace training.1600000.processed.noemoticon.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
          ]
        }
      ],
      "source": [
        "!wget http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\n",
        "!unzip trainingandtestdata.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "vHUJw6dwfe1e"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout, RNN\n",
        "from keras.models import Sequential\n"
      ],
      "metadata": {
        "id": "OpbvbzUCWEiT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocessing"
      ],
      "metadata": {
        "id": "6BT7iIi9S3_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns = [\"sentiment\",\"id\",\"datetime\",\"query\",\"user\",\"tweet\"]\n",
        "df = pd.read_csv(\"/content/training.1600000.processed.noemoticon.csv\", encoding=\"latin-1\", names=columns)"
      ],
      "metadata": {
        "id": "DTEMjI9bV3WR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"sentiment\"].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sjx0Ut-RX1qg",
        "outputId": "65c05e23-79dd-4188-fb72-03cfe54f5ec7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    800000\n",
              "4    800000\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.loc[:,[\"sentiment\",\"tweet\"]]\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "R8dNu3B4eJiy",
        "outputId": "4e6e58c1-51d0-44fc-df31-fe24f4a0ce9d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         sentiment                                              tweet\n",
              "0                0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
              "1                0  is upset that he can't update his Facebook by ...\n",
              "2                0  @Kenichan I dived many times for the ball. Man...\n",
              "3                0    my whole body feels itchy and like its on fire \n",
              "4                0  @nationwideclass no, it's not behaving at all....\n",
              "...            ...                                                ...\n",
              "1599995          4  Just woke up. Having no school is the best fee...\n",
              "1599996          4  TheWDB.com - Very cool to hear old Walt interv...\n",
              "1599997          4  Are you ready for your MoJo Makeover? Ask me f...\n",
              "1599998          4  Happy 38th Birthday to my boo of alll time!!! ...\n",
              "1599999          4  happy #charitytuesday @theNSPCC @SparksCharity...\n",
              "\n",
              "[1600000 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-88875814-364c-4a47-b74b-62f57caae871\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>is upset that he can't update his Facebook by ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599995</th>\n",
              "      <td>4</td>\n",
              "      <td>Just woke up. Having no school is the best fee...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599996</th>\n",
              "      <td>4</td>\n",
              "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599997</th>\n",
              "      <td>4</td>\n",
              "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599998</th>\n",
              "      <td>4</td>\n",
              "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599999</th>\n",
              "      <td>4</td>\n",
              "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1600000 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-88875814-364c-4a47-b74b-62f57caae871')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-88875814-364c-4a47-b74b-62f57caae871 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-88875814-364c-4a47-b74b-62f57caae871');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "samples_fraction = 0.2\n",
        "stratify_on = \"sentiment\"\n",
        "\n",
        "_, dataset = train_test_split(df, stratify=df[stratify_on], \n",
        "                                 test_size=samples_fraction,\n",
        "                                 random_state=76)\n",
        "\n",
        "# Reset the index of the sampled DataFrame\n",
        "dataset.reset_index(drop=True, inplace=True)\n"
      ],
      "metadata": {
        "id": "yiqqBRMLZlS-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"sentiment\"].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gmz4HeOLdsBK",
        "outputId": "2b197f55-49bb-4864-eb34-dae6032a5051"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4    160000\n",
              "0    160000\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "def preprocess_tweet(tweet, remove_stopwords=True, stem_words=True, handle_negation=True):\n",
        "    # Convert to lowercase\n",
        "    tweet = tweet.lower()\n",
        "    \n",
        "    # Remove URLs, mentions, and hashtags\n",
        "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+|\\@\\w+|\\#\\w+\", \"\", tweet)\n",
        "    \n",
        "    # Remove punctuation\n",
        "    tweet = re.sub(r'[^\\w\\s]', '', tweet)\n",
        "    \n",
        "    # Remove numbers\n",
        "    tweet = re.sub(r'\\d+', '', tweet)\n",
        "    \n",
        "    # Remove stopwords if specified\n",
        "    if remove_stopwords:\n",
        "        tweet_tokens = tweet.split()\n",
        "        tweet = \" \".join([word for word in tweet_tokens if word not in stop_words])\n",
        "    \n",
        "    # Stem words if specified\n",
        "    if stem_words:\n",
        "        stemmer = PorterStemmer()\n",
        "        tweet_tokens = tweet.split()\n",
        "        tweet = \" \".join([stemmer.stem(word) for word in tweet_tokens])\n",
        "        \n",
        "    # Handle negation if specified\n",
        "    if handle_negation:\n",
        "        tweet_tokens = word_tokenize(tweet)\n",
        "        negation_flag = False\n",
        "        negated_tokens = []\n",
        "        for i, token in enumerate(tweet_tokens):\n",
        "            if token in [\"not\", \"n't\", \"no\"]:\n",
        "                negation_flag = True\n",
        "            elif negation_flag:\n",
        "                negated_tokens.append(\"not_\" + token)\n",
        "                negation_flag = False\n",
        "            else:\n",
        "                negated_tokens.append(token)\n",
        "        tweet = \" \".join(negated_tokens)\n",
        "    \n",
        "    return tweet\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbpGmQs3sluy",
        "outputId": "2e3fb318-7ac5-4ac6-d916-31ddd38dee22"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "\n",
        "dataset[\"tweet\"] = dataset[\"tweet\"].progress_apply(preprocess_tweet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o55eEstft44G",
        "outputId": "d6cbbf5c-ecac-4d71-f9d7-a5440a493a64"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320000/320000 [01:27<00:00, 3664.59it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "X = dataset[\"tweet\"]\n",
        "y = dataset[\"sentiment\"]"
      ],
      "metadata": {
        "id": "b1bs0urOkyfX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = y.replace(4,1)\n",
        "y = y.replace(4,1)"
      ],
      "metadata": {
        "id": "5oOVFfxJWkkV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "del df\n",
        "gc.collect() # collect garbage "
      ],
      "metadata": {
        "id": "_SGNl2V2fGGv",
        "outputId": "f8a99d9a-c0ef-49a3-b649-2a35e418c1a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "43"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=76)\n",
        "\n",
        "y_train = to_categorical(y_train, num_classes=2)\n",
        "y_test = to_categorical(y_test, num_classes=2)\n"
      ],
      "metadata": {
        "id": "JrQsMehAfdrS"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 15000\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "tokenizer.fit_on_texts(X_train)"
      ],
      "metadata": {
        "id": "QUQNR7itm6Oc"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)"
      ],
      "metadata": {
        "id": "j2BDoz4M6j3g"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad the sequences to a fixed length\n",
        "max_length = 150\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=max_length)\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=max_length)\n"
      ],
      "metadata": {
        "id": "PKLPxVF17AP4"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GloVe RNN"
      ],
      "metadata": {
        "id": "m1v5XpXUStEz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process and Train"
      ],
      "metadata": {
        "id": "rTs3GS0UTz6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle ;\n",
        "!mkdir ~/.kaggle/ ;\n",
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/ ;\n",
        "!chmod 600 ~/.kaggle/kaggle.json ;\n",
        "!kaggle datasets download -d rtatman/glove-global-vectors-for-word-representation\n",
        "!unzip  /content/glove-global-vectors-for-word-representation.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJjBfkDE9NZv",
        "outputId": "d91f7ebf-a16c-41bb-932f-4bf53d2fa30f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.13)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2022.12.7)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.65.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.26.15)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mTraceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3108, in _dep_map\n",
            "    return self.__dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2901, in __getattr__\n",
            "    raise AttributeError(attr)\n",
            "AttributeError: _DistInfoDistribution__dep_map\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 169, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 242, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 441, in run\n",
            "    conflicts = self._determine_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 572, in _determine_conflicts\n",
            "    return check_install_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 101, in check_install_conflicts\n",
            "    package_set, _ = create_package_set_from_installed()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 42, in create_package_set_from_installed\n",
            "    dependencies = list(dist.iter_dependencies())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/metadata/pkg_resources.py\", line 216, in iter_dependencies\n",
            "    return self._dist.requires(extras)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2821, in requires\n",
            "    dm = self._dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3110, in _dep_map\n",
            "    self.__dep_map = self._compute_dependencies()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3120, in _compute_dependencies\n",
            "    reqs.extend(parse_requirements(req))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3173, in __init__\n",
            "    super(Requirement, self).__init__(requirement_string)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/requirements.py\", line 102, in __init__\n",
            "    req = REQUIREMENT.parseString(requirement_string)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 1131, in parse_string\n",
            "    loc, tokens = self._parse(instring, 0)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4375, in parseImpl\n",
            "    return self.expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4891, in parseImpl\n",
            "    return super().parseImpl(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4790, in parseImpl\n",
            "    loc, tokens = self_expr_parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line -1, in parseImpl\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 79, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
            "    return run(options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 207, in exc_logging_wrapper\n",
            "    logger.debug(\"Exception information:\", exc_info=True)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1465, in debug\n",
            "    self._log(DEBUG, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1624, in _log\n",
            "    self.handle(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1634, in handle\n",
            "    self.callHandlers(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1696, in callHandlers\n",
            "    hdlr.handle(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 968, in handle\n",
            "    self.emit(record)\n",
            "  File \"/usr/lib/python3.10/logging/handlers.py\", line 75, in emit\n",
            "    logging.FileHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1218, in emit\n",
            "    StreamHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1100, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 943, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/logging.py\", line 112, in format\n",
            "    formatted = super().format(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 686, in format\n",
            "    record.exc_text = self.formatException(record.exc_info)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 636, in formatException\n",
            "    traceback.print_exception(ei[0], ei[1], tb, None, sio)\n",
            "  File \"/usr/lib/python3.10/traceback.py\", line 119, in print_exception\n",
            "    te = TracebackException(type(value), value, tb, limit=limit, compact=True)\n",
            "  File \"/usr/lib/python3.10/traceback.py\", line 502, in __init__\n",
            "    self.stack = StackSummary.extract(\n",
            "  File \"/usr/lib/python3.10/traceback.py\", line 383, in extract\n",
            "    f.line\n",
            "  File \"/usr/lib/python3.10/traceback.py\", line 306, in line\n",
            "    self._line = linecache.getline(self.filename, self.lineno)\n",
            "  File \"/usr/lib/python3.10/linecache.py\", line 30, in getline\n",
            "    lines = getlines(filename, module_globals)\n",
            "  File \"/usr/lib/python3.10/linecache.py\", line 46, in getlines\n",
            "    return updatecache(filename, module_globals)\n",
            "  File \"/usr/lib/python3.10/linecache.py\", line 136, in updatecache\n",
            "    with tokenize.open(fullname) as fp:\n",
            "  File \"/usr/lib/python3.10/tokenize.py\", line 396, in open\n",
            "    encoding, lines = detect_encoding(buffer.readline)\n",
            "  File \"/usr/lib/python3.10/tokenize.py\", line 365, in detect_encoding\n",
            "    first = read_or_stop()\n",
            "  File \"/usr/lib/python3.10/tokenize.py\", line 323, in read_or_stop\n",
            "    return readline()\n",
            "KeyboardInterrupt\n",
            "^C\n",
            "mkdir: cannot create directory ‘/root/.kaggle/’: File exists\n",
            "glove-global-vectors-for-word-representation.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Archive:  /content/glove-global-vectors-for-word-representation.zip\n",
            "replace glove.6B.100d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Use one-hot vectors for one option\n",
        "# X_train_onehot = tokenizer.sequences_to_matrix(X_train_seq, mode='binary')\n",
        "# X_test_onehot = tokenizer.sequences_to_matrix(X_test_seq, mode='binary')\n",
        "\n",
        "# Use GloVe embeddings for another option\n",
        "glove_embeddings_index = {}\n",
        "with open('glove.6B.200d.txt') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        glove_embeddings_index[word] = coefs\n",
        "embedding_dim = 200\n",
        "glove_embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i >= vocab_size:\n",
        "        break\n",
        "    embedding_vector = glove_embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        glove_embedding_matrix[i] = embedding_vector\n"
      ],
      "metadata": {
        "id": "H7g5llbf6MSY",
        "outputId": "a899236b-7b6f-42b6-90f2-fdb77a66b916",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-c3aa985a11f4>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mglove_embeddings_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'glove.6B.200d.txt'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.layers import CuDNNLSTM, Dropout,SimpleRNN\n",
        "\n",
        "filepath = 'my_best_model.hdf5'\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",patience=5,restore_best_weights=True)\n",
        "checkpoint = ModelCheckpoint(filepath=filepath, \n",
        "                             monitor='val_loss',\n",
        "                             verbose=1, \n",
        "                             save_best_only=True,\n",
        "                             mode='min')\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "\n",
        "\n",
        "RNN_glove = Sequential()\n",
        "RNN_glove.add(Embedding(vocab_size, embedding_dim, input_length=max_length, weights=[glove_embedding_matrix], trainable=False))\n",
        "RNN_glove.add(SimpleRNN(128))\n",
        "RNN_glove.add(Dropout(0.2))\n",
        "RNN_glove.add(Dense(2, activation='softmax'))\n",
        "RNN_glove.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "\n",
        "hist = RNN_glove.fit(X_train_pad, y_train, epochs=100, batch_size=256, validation_split=0.2, callbacks=[checkpoint,callback])\n"
      ],
      "metadata": {
        "id": "qWVr4ep6-Y4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results"
      ],
      "metadata": {
        "id": "XA43rbnBTwaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = RNN_glove.predict(X_test_pad)"
      ],
      "metadata": {
        "id": "8kDS-oQkpWnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true_classes = np.argmax(y_test, axis=1)\n",
        "conf_mat = confusion_matrix(y_true_classes, y_pred_classes)\n",
        "disp = ConfusionMatrixDisplay(conf_mat, display_labels=['Negative', 'Positive'])\n",
        "disp.plot()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jhdpH5mAp7mN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "target_names = [\"Negative\",\"Positive\"]\n",
        "print(classification_report(y_true_classes, y_pred_classes, target_names=target_names))\n"
      ],
      "metadata": {
        "id": "dojnIvNCrPhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word2Vec RNN"
      ],
      "metadata": {
        "id": "32W-iBeMAJTn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process and Train"
      ],
      "metadata": {
        "id": "yVocKf1sTr6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tqdm.pandas()\n",
        "dataset[\"tokenized\"] = dataset[\"tweet\"].progress_apply(word_tokenize)"
      ],
      "metadata": {
        "id": "otj-XhfY3mCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"tokenized\"]"
      ],
      "metadata": {
        "id": "emOZhFK2_dj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "\n",
        "w2v_model = Word2Vec(dataset[\"tokenized\"].to_numpy(), vector_size=embedding_dim, window=5, min_count=1, workers=4)\n"
      ],
      "metadata": {
        "id": "Kj3nFlNR_Sng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i >= vocab_size:\n",
        "        break\n",
        "    try: \n",
        "        embedding_vector = w2v_model.wv.get_vector(word)\n",
        "    except KeyError:\n",
        "        print(f\"the word {word} is not available\")\n",
        "    if embedding_vector is not None:\n",
        "        w2v_embedding_matrix[i] = embedding_vector\n"
      ],
      "metadata": {
        "id": "6RFQltnPBcw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fx84gUrMHqNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filepath = 'my_best_model.hdf5'\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",patience=5,restore_best_weights=True)\n",
        "checkpoint = ModelCheckpoint(filepath=filepath, \n",
        "                             monitor='val_loss',\n",
        "                             verbose=1, \n",
        "                             save_best_only=True,\n",
        "                             mode='min')\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "\n",
        "\n",
        "RNN_w2v = Sequential()\n",
        "RNN_w2v.add(Embedding(vocab_size, embedding_dim, input_length=max_length, weights=[w2v_embedding_matrix], trainable=False))\n",
        "RNN_w2v.add(SimpleRNN(128))\n",
        "RNN_w2v.add(Dropout(0.2))\n",
        "RNN_w2v.add(Dense(2, activation='softmax'))\n",
        "RNN_w2v.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "\n",
        "hist = RNN_w2v.fit(X_train_pad, y_train, epochs=100, batch_size=256, validation_split=0.2, callbacks=[checkpoint,callback])\n"
      ],
      "metadata": {
        "id": "3qlBKa-xBnqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = RNN_w2v.predict(X_test_pad)"
      ],
      "metadata": {
        "id": "h1OWi4qMIP8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results"
      ],
      "metadata": {
        "id": "6ls4J6RkTl8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true_classes = np.argmax(y_test, axis=1)\n",
        "conf_mat = confusion_matrix(y_true_classes, y_pred_classes)\n",
        "disp = ConfusionMatrixDisplay(conf_mat, display_labels=['Negative', 'Positive'])\n",
        "disp.plot()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qKFpu32dISS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "target_names = [\"Negative\",\"Positive\"]\n",
        "print(classification_report(y_true_classes, y_pred_classes, target_names=target_names))\n"
      ],
      "metadata": {
        "id": "rqRTUJJNIVXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#One-Hot RNN"
      ],
      "metadata": {
        "id": "DpOi0BtBSA9M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process and Train"
      ],
      "metadata": {
        "id": "vPsN6e7hT7QP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Define a sample text data\n",
        "\n",
        "# Initialize the CountVectorizer\n",
        "vectorizer = CountVectorizer(binary=True, max_features=vocab_size )\n",
        "\n",
        "# Fit the vectorizer to the text data\n",
        "vectorizer.fit(X_train)\n",
        "\n",
        "# Transform the text data into a one-hot encoded matrix\n",
        "X_train_onehot = vectorizer.transform(X_train)\n",
        "X_test_onehot = vectorizer.transform(X_test)\n",
        "\n",
        "# Print the one-hot encoded data\n",
        "type(X_train_onehot)"
      ],
      "metadata": {
        "id": "Sbhsxu5gSUew",
        "outputId": "7495fab7-c33e-46a1-c420-7a168ce79f6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "scipy.sparse._csr.csr_matrix"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    }
  ]
}