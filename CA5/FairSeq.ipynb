{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fairseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:  \n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join all paths to a base path \n",
    "BASE_PATH = \"/content/drive/MyDrive/NLP/CA5\"\n",
    "TOKENIZED_DATA_PATH = os.path.join(BASE_PATH, \"tokenized_data\")\n",
    "TRAIN_DATA_PATH = os.path.join(TOKENIZED_DATA_PATH, \"train\")\n",
    "VALID_DATA_PATH = os.path.join(TOKENIZED_DATA_PATH, \"valid\")\n",
    "TEST_DATA_PATH = os.path.join(TOKENIZED_DATA_PATH, \"test\")\n",
    "VOCAB_SIZE = 10000\n",
    "LEARNING_RATE = 0.0025\n",
    "LABEL_SMOOTHING = 0.2\n",
    "ADAM_BETA11 = 0.9\n",
    "ADAM_BETA22 = 0.98\n",
    "DROPOUT = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./data_bin\n",
    "\n",
    "fairseq_preprocess_command = f\"\"\"\n",
    "    !fairseq-preprocess --source-lang en --target-lang fa \\\n",
    "  --trainpref {TRAIN_DATA_PATH} \\\n",
    "  --validpref {VALID_DATA_PATH} \\\n",
    "  --testpref {TEST_DATA_PATH} \\\n",
    "  --destdir ./data_bin/\n",
    "  \"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$fairseq_preprocess_command\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairseq_train_command = f\"\"\"\n",
    "!fairseq-train \\\n",
    "    \"./data_bin/\" \\\n",
    "    --arch lstm --share-decoder-input-output-embed \\\n",
    "    --optimizer adam --adam-betas '({ADAM_BETA11},{ADAM_BETA22})' --clip-norm 0.0 \\\n",
    "    --lr {LEARNING_RATE} --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n",
    "    --dropout {DROPOUT} --weight-decay 0.0001 \\\n",
    "    --criterion label_smoothed_cross_entropy --label-smoothing {LABEL_SMOOTHING} \\\n",
    "    --max-tokens 4096 \\\n",
    "    --eval-bleu \\\n",
    "    --eval-bleu-args '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}' \\\n",
    "    --eval-bleu-detok moses \\\n",
    "    --eval-bleu-print-samples \\\n",
    "    --best-checkpoint-metric bleu --maximize-best-checkpoint-metric \\\n",
    "    --fp16 --memory-efficient-fp16 \\\n",
    "    --max-epoch 5 \\\n",
    "    --save-dir ./data_bin/checkpoints/ \\\n",
    "    --tensorboard-logdir ./data_bin/logs\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# echo \"$fairseq_train_command\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$fairseq_train_command\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
