{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VavRe/nlp-ut/blob/main/CA5/Tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhKb3XRr7NEF",
        "outputId": "c72a05ce-db89-4951-c60b-8417568d30fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: fairseq in /usr/local/lib/python3.10/dist-packages (0.12.2)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.15.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq) (0.29.34)\n",
            "Requirement already satisfied: hydra-core<1.1,>=1.0.7 in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.0.7)\n",
            "Requirement already satisfied: omegaconf<2.1 in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.0.6)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq) (2022.10.31)\n",
            "Requirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.3.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.0.1+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq) (4.65.0)\n",
            "Requirement already satisfied: bitarray in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.7.4)\n",
            "Requirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.0.2+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.22.4)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/lib/python3.10/dist-packages (from hydra-core<1.1,>=1.0.7->fairseq) (4.8)\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (4.5.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (2.7.0)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.8.10)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (4.9.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.12.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->fairseq) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->fairseq) (16.0.5)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq) (2.21)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->fairseq) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->fairseq) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement sacramentos (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for sacramentos\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece\n",
        "!pip install sacramentos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swex_kah7TF3",
        "outputId": "89407c70-4032-419a-c2a7-5b86003e5783"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:  \n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ZhVjlk257KLd"
      },
      "outputs": [],
      "source": [
        "import sentencepiece as spm\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lq-q4N2x7KLm",
        "outputId": "0b804388-9f91-4be3-dfe0-dd6c3708f685"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wc: ./raw_data/train.fa: No such file or directory\n",
            "wc: ./raw_data/train.en: No such file or directory\n",
            "wc: ./raw_data/valid.fa: No such file or directory\n",
            "wc: ./raw_data/valid.en: No such file or directory\n",
            "wc: ./raw_data/test.fa: No such file or directory\n",
            "wc: ./raw_data/test.fa: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "## Here, you can see how many lines each file has using unix\n",
        "\n",
        "!wc -l ./raw_data/train.fa\n",
        "!wc -l ./raw_data/train.en\n",
        "!wc -l ./raw_data/valid.fa\n",
        "!wc -l ./raw_data/valid.en\n",
        "!wc -l ./raw_data/test.fa\n",
        "!wc -l ./raw_data/test.fa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ID-CxwnT7KLp",
        "outputId": "7728e566-db87-45f9-f343-0dab458ce820"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "head: cannot open './raw_data/train.fa' for reading: No such file or directory\n",
            "head: cannot open './raw_data/train.en' for reading: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!head ./raw_data/train.fa -n 3\n",
        "!head ./raw_data/train.en -n 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "vwF3I1SW7KLq"
      },
      "outputs": [],
      "source": [
        "#join all paths to a base path \n",
        "BASE_PATH = \"/content/drive/MyDrive/NLP/CA5\"\n",
        "RAW_DATA_PATH = os.path.join(BASE_PATH, \"raw_data\")\n",
        "TOKENIZED_DATA_PATH = os.path.join(BASE_PATH, \"tokenized_data\")\n",
        "TOKENIZER_PATH = os.path.join(BASE_PATH, \"tokenizer\")\n",
        "VOCAB_SIZE = 10000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "dALTt3-47KLs"
      },
      "outputs": [],
      "source": [
        "# Do this only if  m_fa is not created in ./tokenizer\n",
        "\n",
        "INPUT_TRAIN_FA = os.path.join(RAW_DATA_PATH, \"train.fa\")\n",
        "OUTPUT_MODEL_FA = os.path.join(TOKENIZER_PATH, \"m_fa\")\n",
        "\n",
        "spm.SentencePieceTrainer.train(input=INPUT_TRAIN_FA, model_prefix=OUTPUT_MODEL_FA, vocab_size=VOCAB_SIZE, character_coverage=1.0, model_type='bpe')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Ogk6sltI7KLt"
      },
      "outputs": [],
      "source": [
        "INPUT_TRAIN_EN = os.path.join(RAW_DATA_PATH, \"train.en\")\n",
        "OUTPUT_MODEL_EN = os.path.join(TOKENIZER_PATH, \"m_en\")\n",
        "\n",
        "\n",
        "spm.SentencePieceTrainer.train(input=INPUT_TRAIN_EN, model_prefix=OUTPUT_MODEL_EN, vocab_size=VOCAB_SIZE, character_coverage=1.0, model_type='bpe')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfQTLTGy7KLv",
        "outputId": "2704c3c1-b3ff-4117-8366-278feaeb152a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the length of train_fa is 670000 and the length of train_en is 670000\n"
          ]
        }
      ],
      "source": [
        "# Read all raw files as lists of strings by line break\n",
        "with open(os.path.join(RAW_DATA_PATH, \"train.fa\"), \"r\") as f:\n",
        "    train_fa = f.readlines()\n",
        "with open(os.path.join(RAW_DATA_PATH, \"train.en\"), \"r\") as f:\n",
        "    train_en = f.readlines()\n",
        "with open(os.path.join(RAW_DATA_PATH, \"valid.fa\"), \"r\") as f:\n",
        "    valid_fa = f.readlines()\n",
        "with open(os.path.join(RAW_DATA_PATH, \"valid.en\"), \"r\") as f:\n",
        "    valid_en = f.readlines()\n",
        "with open(os.path.join(RAW_DATA_PATH, \"test.fa\"), \"r\") as f:\n",
        "    test_fa = f.readlines()\n",
        "with open(os.path.join(RAW_DATA_PATH, \"test.en\"), \"r\") as f:\n",
        "    test_en = f.readlines()\n",
        "    \n",
        "print(f\"the length of train_fa is {len(train_fa)} and the length of train_en is {len(train_en)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "sbtSX6Ms7KLw",
        "outputId": "38216e34-6382-4d26-abd8-6d38935b76e6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'مرگ 50 ستیزه جوی دیگر در عملیات وزیرستان شمالی\\n'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_fa[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wCZfHkdD7KLy",
        "outputId": "0e413f13-3575-4ed1-e33a-4437802f617e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'North Waziristan operation kills 50 more militants\\n'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_en[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "gbUfnJsa7KL0"
      },
      "outputs": [],
      "source": [
        "sp_fa = spm.SentencePieceProcessor(model_file=f\"{OUTPUT_MODEL_FA}.model\")\n",
        "sp_en = spm.SentencePieceProcessor(model_file=f\"{OUTPUT_MODEL_EN}.model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7pkym927KL2",
        "outputId": "2a5e8cd2-5bca-4fef-8b42-56d25e9d61c9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['▁مرگ',\n",
              " '▁50',\n",
              " '▁ستیزه',\n",
              " '▁جوی',\n",
              " '▁دیگر',\n",
              " '▁در',\n",
              " '▁عملیات',\n",
              " '▁وزیرستان',\n",
              " '▁شمالی']"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoded_train_fa = sp_fa.encode(train_fa, out_type=str)\n",
        "encoded_train_en = sp_en.encode(train_en, out_type=str)\n",
        "encoded_valid_fa = sp_fa.encode(valid_fa, out_type=str)\n",
        "encoded_valid_en = sp_en.encode(valid_en, out_type=str)\n",
        "encoded_test_fa = sp_fa.encode(test_fa, out_type=str)\n",
        "encoded_test_en = sp_en.encode(test_en, out_type=str)\n",
        "\n",
        "encoded_train_fa[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhvzeYkV7KL3",
        "outputId": "0c021580-27de-4b37-b065-15ae54240044"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['▁مرگ',\n",
              " '▁50',\n",
              " '▁ستیزه',\n",
              " '▁جوی',\n",
              " '▁دیگر',\n",
              " '▁در',\n",
              " '▁عملیات',\n",
              " '▁وزیرستان',\n",
              " '▁شمالی']"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoded_train_fa[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DmMz9Z427KL5",
        "outputId": "0d15b856-bac5-46af-b023-b169e30fd60e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'▁مرگ ▁50 ▁ستیزه ▁جوی ▁دیگر ▁در ▁عملیات ▁وزیرستان ▁شمالی'"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# turn each list of strings into a single string with space as the delimiter\n",
        "encoded_train_fa_str = [' '.join(sent) for sent in encoded_train_fa]\n",
        "encoded_train_en_str = [' '.join(sent) for sent in encoded_train_en]\n",
        "encoded_valid_fa_str = [' '.join(sent) for sent in encoded_valid_fa]\n",
        "encoded_valid_en_str = [' '.join(sent) for sent in encoded_valid_en]\n",
        "encoded_test_fa_str = [' '.join(sent) for sent in encoded_test_fa]\n",
        "encoded_test_en_str = [' '.join(sent) for sent in encoded_test_en]\n",
        "\n",
        "encoded_train_fa_str[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "2qUzw6et7KL6"
      },
      "outputs": [],
      "source": [
        "# write all arrays of last cell to files in ./tokenized_data\n",
        "\n",
        "with open(os.path.join(TOKENIZED_DATA_PATH, \"train.fa\"), \"w\") as f:\n",
        "    f.writelines(encoded_train_fa_str)\n",
        "with open(os.path.join(TOKENIZED_DATA_PATH, \"train.en\"), \"w\") as f:\n",
        "    f.writelines(encoded_train_en_str)\n",
        "with open(os.path.join(TOKENIZED_DATA_PATH, \"valid.fa\"), \"w\") as f:\n",
        "    f.writelines(encoded_valid_fa_str)\n",
        "with open(os.path.join(TOKENIZED_DATA_PATH, \"valid.en\"), \"w\") as f:\n",
        "    f.writelines(encoded_valid_en_str)\n",
        "with open(os.path.join(TOKENIZED_DATA_PATH, \"test.fa\"), \"w\") as f:\n",
        "    f.writelines(encoded_test_fa_str)\n",
        "with open(os.path.join(TOKENIZED_DATA_PATH, \"test.en\"), \"w\") as f:\n",
        "    f.writelines(encoded_test_en_str)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "yCuGCGfI7KL7"
      },
      "outputs": [],
      "source": [
        "# show_tokenized_top_fa_command = f\"head {TOKENIZED_DATA_PATH}/train.fa -n 5\"\n",
        "# show_tokenized_top_en_command = f\"head {TOKENIZED_DATA_PATH}/train.en -n 5\"\n",
        "# !{show_tokenized_top_en_command} \n",
        "# !{show_tokenized_top_fa_command}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "9VI2zsUK7KL7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
