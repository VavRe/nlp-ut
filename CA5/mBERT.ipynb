{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"/content/drive/MyDrive/NLP/CA5\"\n",
    "RAW_DATA_PATH = os.path.join(BASE_PATH, \"raw_data\")\n",
    "TOKENIZED_DATA_PATH = os.path.join(BASE_PATH, \"tokenized_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "text = \"This is a test\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read every file inside raw_data directory seperately and split using line breaks and save it to corresponding variable\n",
    "\n",
    "with open(os.path.join(RAW_DATA_PATH, \"train.en\"), 'r') as f:\n",
    "    train_en = f.read().splitlines()\n",
    "\n",
    "with open(os.path.join(RAW_DATA_PATH, \"train.fa\"), 'r') as f:\n",
    "    train_fa = f.read().splitlines()\n",
    "\n",
    "with open(os.path.join(RAW_DATA_PATH, \"valid.en\"), 'r') as f:\n",
    "    valid_en = f.read().splitlines()\n",
    "\n",
    "with open(os.path.join(RAW_DATA_PATH, \"valid.fa\"), 'r') as f:\n",
    "    valid_fa = f.read().splitlines()\n",
    "\n",
    "with open(os.path.join(RAW_DATA_PATH, \"test.en\"), 'r') as f:\n",
    "    test_en = f.read().splitlines()\n",
    "\n",
    "with open(os.path.join(RAW_DATA_PATH, \"test.fa\"), 'r') as f:\n",
    "    test_fa = f.read().splitlines()\n",
    "\n",
    "\n",
    "\n",
    "# Now for each file iterate over the list and perform tokenizer function on each line and save it to corresponding variable and add description to each running tqdm loop\n",
    "\n",
    "train_en_tokenized = [tokenizer(line, return_tensors='pt').input_ids for line in tqdm(train_en, desc=\"train_en\")]\n",
    "train_fa_tokenized = [tokenizer(line, return_tensors='pt').input_ids for line in tqdm(train_fa, desc=\"train_fa\")]\n",
    "valid_en_tokenized = [tokenizer(line, return_tensors='pt').input_ids for line in tqdm(valid_en, desc=\"valid_en\")]\n",
    "valid_fa_tokenized = [tokenizer(line, return_tensors='pt').input_ids for line in tqdm(valid_fa, desc=\"valid_fa\")]\n",
    "test_en_tokenized = [tokenizer(line, return_tensors='pt').input_ids for line in tqdm(test_en, desc=\"test_en\")]\n",
    "test_fa_tokenized = [tokenizer(line, return_tensors='pt').input_ids for line in tqdm(test_fa, desc=\"test_fa\")]\n",
    "\n",
    "\n",
    "\n",
    "# Now write the tokenized data to files in mBert_tokenized_data directory, each item will be seperated by line break\n",
    "\n",
    "with open(os.path.join(TOKENIZED_DATA_PATH, \"train.en\"), 'w') as f:\n",
    "    f.writelines(train_en_tokenized)\n",
    "\n",
    "with open(os.path.join(TOKENIZED_DATA_PATH, \"train.fa\"), 'w') as f:\n",
    "    f.writelines(train_fa_tokenized)\n",
    "\n",
    "with open(os.path.join(TOKENIZED_DATA_PATH, \"valid.en\"), 'w') as f:\n",
    "    f.writelines(valid_en_tokenized)\n",
    "\n",
    "with open(os.path.join(TOKENIZED_DATA_PATH, \"valid.fa\"), 'w') as f:\n",
    "    f.writelines(valid_fa_tokenized)\n",
    "\n",
    "with open(os.path.join(TOKENIZED_DATA_PATH, \"test.en\"), 'w') as f:\n",
    "    f.writelines(test_en_tokenized)\n",
    "\n",
    "with open(os.path.join(TOKENIZED_DATA_PATH, \"test.fa\"), 'w') as f:\n",
    "    f.writelines(test_fa_tokenized)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
